---------------------------------------------install-----------------------------------------------
*vim /etc/hostname
*vi /etc/hosts
*systemctl stop firewalld && systemctl disable firewalld
*sestatus:disabled
*yum install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9-3.el7 containerd.io
*cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF
*yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes*
*systemctl enable kubelet && systemctl start kubelet*
*cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
*kubeadm config print init-defaults > kubeadm-init.yaml
将advertiseAddress: 1.2.3.4修改为本机地址
将imageRepository: k8s.gcr.io修改为imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
*kubeadm config images pull --config kubeadm-init.yaml
*kubeadm init --config kubeadm-init.yaml
*kubeadm join 192.168.110.128:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:62e5a13e71cbd6b51a71fb19799c4af651ce9004e60ba6bdbda9bdd7f7b699ca 
*mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
*wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml
cat kubeadm-init.yaml | grep serviceSubnet:   将192.168.0.0/16修改为10.96.0.0/12
kubectl apply -f calico.yaml
*kubectl get node
kubectl get pods --all-namespaces
查看token：kubeadm token list
生成token：kubeadm token create
获取CA证书hash：openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
排错：journalctl -f -u kubelet;kubectl describe nodes;systemctl status kubelet
卸载节点：kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
删除节点：kubectl delete node <node name>
重置节点：kubeadm reset
*kubectl describe cs
vi /etc/kubernetes/manifests/kube-scheduler.yaml   和  /kube-controller-manager.yaml  去掉--port=0
systemctl restart kubelet
----------------------------------------概念------------------------------------
*Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是Kubernetes 调度的基本单位
*Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上。Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如Service 用 label 来选择一组 Pod）。Label Selector支持以下几种方式：
等式，如app=nginx和env!=production
集合，如env in (production, qa)
多个label（它们之间是AND关系），如app=nginx,env=test
*Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services,deployments 等都是属于某一个 namespace 的（默认是default），而 Node, PersistentVolumes 等则不属于任何 namespace。
*Deployment 需要指定两个东西：
Pod模板：用来创建 Pod 副本的模板
Label标签：Deployment 需要监控的 Pod 的标签。
*Service 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些endpoints 上。每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。
----------------------------------------组件-----------------------------------------------
*Master：Master 节点是 Kubernetes 集群的控制节点，负责整个集群的管理和控制。Master 节点上包含以下组件：
kube-apiserver：集群控制的入口，提供 HTTP REST 服务。负责 etcd 存储的所有操作
kube-controller-manager：Kubernetes 集群中所有资源对象的自动化控制中心，比如故障检测、自动扩展、滚动更新等；
kube-scheduler：负责 Pod 的调度，按照预定的调度策略将 Pod 调度到相应的机器上
etcd 保存了整个集群的状态，就是一个数据库
*Node 节点上包含以下组件：
kubelet：负责 Pod 的创建、启动、监控、重启、销毁等工作，同时与 Master 节点协作，实现集群管理的基本功能，同时也负责 Volume（CSI）和网络（CNI）的管理；
kube-proxy：为 Service 提供 cluster 内部的服务发现和负载均衡。负责随时与 apiserver 进行通信，apiserver 需要保存各个 node 信息，它需要保存在 etcd 中。kube-proxy 一旦发现某个 service 后端的 pod 地址发生改变，那么就由 kube-proxy 负责在本地将地址写入 iptables 或者 ipvs 规则中。
*ReplicaSet：是 Pod 副本的抽象，用于解决 Pod 的扩容和伸缩
*Deployment：Deployment 表示部署，在内部使用ReplicaSet 来实现。可以通过 Deployment 来生成相应的 ReplicaSet 完成 Pod 副本的创建
*Service 定义了服务的访问入口，服务的调用者通过这个地址访问 Service 后端的 Pod 副本实例。Service 通过 Label Selector 同后端的 Pod 副本建立关系。它是一个 iptables dnat 规则，或者 ipvs 规则，所以是 ping 不通的,可以通过 Service 的名称解析为 Service 的 IP 地址一般格式: svcname.namespace.svc.cluster.local
*kube-dns 负责为整个集群提供 DNS 服务
*Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）
*Ingress Controller 为服务提供外网入口
*Heapster 提供资源监控
*Dashboard 提供 GUI
-------------------------------------Controller------------------------------------
*RelicationController:控制同一类 POD 对象的副本数量，实现程序的滚动更新，或者回滚的操作。在滚动更新时候，允许临时超出规定的副本数量，
*RelicaSet:副本集控制器，它不直接使用，它有一个声明式中心控制器 Deployment
*Deployment:它只能管理无状态的应用，这个控制器，支持二级控制器，例如：HPA（Horizontal Pod Autoscaler，水平 POD 自动伸缩控制器），当负载高的时候，自动启动更多的 POD。
*StatefulSet:管理有状态的应用
*DaemonSet:如果需要在每一个 node 上运行一个副本，而不是随意运行
*Job:运行一次性作业，时间不固定的操作，例如：备份、清理，临时启动一个 POD 来进行备份的任务，运行完成就结束了。如果运行时候 JOB 挂了，那么需要重新启动起来，如果运行完成了则不需要再启动了。
*Cronjob:运行周期性作业
--------------------------------------网络模型-----------------------------------------
*POD网络、集群网络、节点网络
*请求首先到达 node 网络，然后 node 网络代理至 service 网络，service 根据 iptables/ipvs 规则来转发到 pod 网络中的 pod 上。 
*通信方式：
同一个 POD 内的多个容器间的通信，可以通过 lo 通信直接通讯。
POD 与 POD 通信，如果使用 flannel 所有 POD 都处于一个网络，可以跨 node 与另外的 POD 直接通信，因为使用了叠加网络。
POD 与 Service 通信。
--------------------------------------dashboard------------------------------------
*kubectl apply -f https://kuboard.cn/install-script/k8s-dashboard/v2.0.0-beta5.yaml
*kubectl apply -f dashboard-adminuser.yaml
*grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt
*grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key
*openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"
*浏览器导入证书
*https://192.168.122.168:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login
*kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
--------------------------------------命令------------------------------
*kubectl cluster-info
*kubectl get componentstatuses
*kubectl get pods -n kube-system -o wide
*kubectl:
基本命令：
  create         从文件或标准输入创建资源   kubectl create -f file.yaml
  expose         获取一个复制控制器, 服务, 部署或者暴露一个 POD 将其作为新的 Kubernetes 服务公开
  run            创建并运行特定的镜像, 创建使用 deployment 或 job 管理的容器
  set            设置对象的特定功能, 例如发布, 每次去set 不用的image tag
  explain        文档或者资源, 可以用来查看资源清单写法
  get            显示一个或多个资源        kubectl get pod -o wide;kubectl get deployment;kubectl get pods --show-labels;kubectl get rc
  edit           编辑服务器上的资源
  delete         按文件名, 标准输入, 资源和名称或资源和标签选择器删除资源        kubectl delete pods nginx
部署命令:
  rollout        管理资源的部署
  scale          为部署设置新大小, ReplicaSet, Replication Controller, Job
  autoscale      自动扩展一个部署, ReplicaSet, 或者 ReplicationController
群集管理命令:
  certificate    修改证书资源
  cluster-info   显示群集信息
  top            显示资源(CPU / 内存/ 存储)使用情况, 需要安装metrics-server
  cordon         将节点标记为不可调度
  uncordon       将节点标记为可调度
  drain          设定 node 进入维护模式
  taint          更新一个或多个节点上的污点
故障排除和调试命令: 
  describe       显示特定资源或资源组的详细信息    kubectl describe pod nginx
  logs           在容器中打印容器的日志         kubectl logs nginx
  attach         附加到正在运行的容器
  exec           在容器中执行命令    kubectl exec nginx ls
  port-forward   将一个或多个本地端口转发到 pod
  proxy          运行代理到 Kubernetes API 服务器
  cp             将文件和目录复制到容器, 和从容器复制, 跨容器复制文件
  auth           检查授权
高级命令:
  diff           针对将要应用的版本的 Diff 实时版本
  apply          通过文件名或标准输入将配置应用于资源
  patch          使用策略合并补丁更新资源的字段
  replace        用文件名或标准输入替换资源
  wait           实验阶段命令: 在一个或多个资源上等待特定条件, 定义一个触发器
  convert        在不同的API版本之间转换配置文件
  kustomize      从目录或远程 URL 构建 kustomization 目标
设置命令:
  label          更新资源上的标签
  annotate       更新资源上的注释
  completion     命令补全相关功能
其他命令:
  api-resources  在服务器上打印支持的API资源
  api-versions   以 "group/version" 的形式在服务器上打印支持的API版本
  config         修改 kubeconfig 文件
  plugin         提供与插件交互的实用程序
  version        打印客户端和服务器版本信息
*run:
kubectl run --image=nginx:alpine nginx-app --port=80
kubectl run nginx --image=nginx --replicas=5  # 启动 5 个 POD
kubectl run nginx --image=nginx --command -- <cmd> <arg1> ... <argN>
kubectl run pi --schedule="0/5 * * * ?" --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'
kubectl run nginx-deploy --image=nginx --port=80 --replicas=1 --dry-run=true   #测试模式
*expose:
kubectl get deployments
kubectl delete deployment nginx-deployment
kubectl expose deployment nginx-deployment --name=nginx --port=80 --target-port=80 --protocol=TCP
kubectl expose deployment nginx-deployment --port=80 --target-port=80 --type=NodePort  #创建service
kubectl get service
kubectl describe service nginx-deployment
内部访问：nodeip+nodeport  或 VIP  或 Endpoint    外部访问：nodeip+nodeport
kubectl delete service nginx-deployment
*kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir
*kubectl exec -it nginx-58cd4d4f44-8pwb7 -- /bin/bash
*kubectl port-forward --address 0.0.0.0 service/nginx 8888 8111
*coredns:
kubectl get pods -n kube-system -o wide |grep coredns
kubectl get service -n kube-system
kubectl run client --image=busybox --replicas=1 -it --restart=Never
cat /etc/resolv.conf
nameserver 10.96.0.10                                               # kube-dns 地址
search default.svc.cluster.local svc.cluster.local cluster.local    # 默认的解析搜索域
options ndots:5
wget -O - -q http://nginx:80/
*label:
kubectl get pods --show-labels -l app=nginx
kubectl label pods client release=canary
kubectl label pods client release=stable --overwrite
kubectl label nodes node2 disktype-   #删除标签
kubectl label nodes node2 disktype=ssd
*动态扩容：kubectl scale --replicas=5 deployment nginx-deployment
*滚动升级：kubectl set image deployment nginx-deployment nginx=ikubernetes/myapp:v2  
kubectl rollout status deployment nginx-deployment
kubectl rollout undo deployment nginx-deployment  
*kubectl edit service nginx
*kubectl edit deployment nginx-deployment  
type: ClusterIP -> type: NodePort
*kubectl logs redis
*kubectl exec -it redis -c myapp -- /bin/sh
*资源限制：kubectl set resources deployment nginx-deployment -c=nginx --limits=cpu=500m,memory=128Mi  -c指定容器
-------------------------------------------------yaml----------------------------------
*apiVersion，这里它的值是 v1，这个版本号需要根据我们安装的 kubernetes 版本和资源类型进行变化的，记住不是写死的
*kind，这里我们创建的是一个 Pod，当然根据你的实际情况，这里资源类型可以是 Deployment、Job、Ingress、Service 等待。
*metadata：包含了我们定义的 Pod 的一些 meta 信息，比如名称、namespace、标签等等信息。
*spec：包括一些containers，storage，volumes，或者其他Kubernetes需要知道的参数，以及诸如是否在容器失败时重新启动容器的属性。你可以在特定 Kubernetes API 找到完整的 Kubernetes Pod 的属性。
--------------------------------------------------pod-----------------------------
*静态 Pod 直接由特定节点上的kubelet进程来管理，不通过 master 节点上的apiserver,kubelet会自动为每一个静态 pod 在 Kubernetes 的 apiserver 上创建一个镜像 Pod（Mirror Pod)。创建静态 Pod 有两种方式：配置文件和 HTTP 两种方式。
1)进入node，systemctl status kubelet  找到启动配置文件.添加：Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true"   重启kubelet
2)–manifest-url=参数指定的地址
*pod hook:poststart在容器创建后执行，prestop在容器终止前调用。钩子函数实现方式：exec,http
*健康检查：LivenessProbe探测应用是否处于健康状态，如果不健康则删除并重新创建容器。ReadinessProbe探测应用是否启动完成并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量。这两个探针的支持两种配置方式：exec：执行一段命令；http：检测某个 http 请求
kubectl describe pod 查看event
*PostStart和PreStop包括liveness和readiness是属于主容器的生命周期范围内的，而Init Container是独立于主容器之外的，当然他们都属于Pod的生命周期范畴之内的
--------------------------------------------RC-----------------------------
kubectl create -f rc-demo.yaml
kubectl get rc
kubectl describe rc rc-demo
kubectl apply -f rc-demo.yaml  # 修改源文件后执行
kubectl edit rc rc-demo   #动态修改
-------------------------------------------deployment------------------------
*一个Deployment拥有多个Replica Set，而一个Replica Set拥有一个或多个Pod。一个Deployment控制多个rs主要是为了支持回滚机制，每当Deployment操作时，Kubernetes会重新生成一个Replica Set并保留。
kubectl get deployments
kubectl get rs
滚动升级：
minReadySeconds: 5
strategy:
  # indicate which strategy we want for rolling update
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
kubectl apply -f nginx-deployment.yaml
kubectl rollout status deployment/nginx-deploy
kubectl rollout pause deployment nginx-deployment
kubectl rollout resume deployment nginx-deployment
kubectl rollout history deployment nginx-deployment --revision=3
kubectl rollout undo deployment nginx-deployment --to-revision=2
设置Deployment的.spec.revisionHistoryLimit来限制最大保留的revision number
------------------------------------HPA--------------------------------
*HAP通过监控分析RC或者Deployment控制的所有Pod的负载变化情况来确定是否需要调整Pod的副本数量
*HPA在kubernetes集群中被设计成一个controller，我们可以简单的通过kubectl autoscale命令来创建一个HPA资源对象，HPA Controller默认30s轮询一次（可通过kube-controller-manager的标志--horizontal-pod-autoscaler-sync-period进行设置）
*Heapster：仅支持CPU使用率
*kubectl autoscale deployment hpa-nginx-deploy --cpu-percent=10 --min=1 --max=10
*kubectl get hpa  
*kubectl get hpa hpa-nginx-deploy -o yaml
---------------------------------------helm--------------------------------
*Helm是一个命令行下的客户端工具。主要用于Kubernetes应用程序Chart的创建、打包、发布及创建和管理
本地和远程的Chart仓库。
*Tiller是Helm的服务端，部署在Kubernetes集群中，Tiller用于接收Helm的请求，并根据Chart生成Kubernete
s的部署文件，然后提交给Kubernetes创建应用，Tiller还提供和了Release的升级、删除、回滚等一系列功能。
*Chart是一个Helm的程序包，包含了运行一个Kubernetes应用程序所需的镜像、依赖关系和资源定义等。
*docker search tiller;docker pull;docker tag 
*kubectl get pod -n kube-system 
*kubectl describe pod tiller-deploy-6cf797ffc4-642w2 -n kube-system
*helm init --service-account tiller --tiller-image gcr.io/kubernetes-helm/tiller:latest --skip-refresh
*kubectl edit deployment tiller-deploy -n kube-system
*helm reset --force
*配置方式：
helm init --client-only --stable-repo-url https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts/
helm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/
helm repo update
helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1  --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm init --service-account tiller --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --tiller-tls-cert /etc/kubernetes/ssl/tiller001.pem --tiller-tls-key /etc/kubernetes/ssl/tiller001-key.pem --tls-ca-cert /etc/kubernetes/ssl/ca.pem --tiller-namespace kube-system --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount
kubectl -n kube-system get pods|grep tiller
------------------------------------------------JOB-------------------------------------
*Job负责处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。而CronJob则就是在Job上加上了时间调度。
*Job的RestartPolicy仅支持Never和OnFailure两种，不支持Always。
*kubectl get jobs
*kubectl logs podname
*crontab的格式如下：分 时 日 月 星期   .spec.successfulJobsHistoryLimit和.spec.failedJobsHistoryLimit推荐填写
*kubectl get cronjob
* kubectl delete cronjob cronjob-demo  
* kubectl delete jobs --all
*kubectl run hello --schedule="*/1 * * * *" --restart=OnFailure --image=busybox -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"
---------------------------------------------service----------------------------------
*Service是一种抽象的对象，它定义了一组Pod的逻辑集合和一个用于访问它们的策略
*Node IP：Node节点的IP地址   Pod IP: Pod的IP地址   Cluster IP: Service的IP地址
*Service能够支持 TCP 和 UDP 协议，默认是 TCP 协议。
*每个Node会运行一个kube-proxy进程, 负责为Service实现一种 VIP(clusterIP）的代理形式，默认是使用的iptables这种模式来代理。kube-proxy会监视Kubernetes master对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会添加上 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某一个个上面。 对于每个 Endpoints 对象，它也会安装 iptables 规则，这个规则会选择一个 backend Pod。
*Service 类型:
ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的ServiceType。
NodePort：通过每个 Node 节点上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 :，可以从集群的外部访问一个 NodePort 服务。
LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务，这个需要结合具体的云厂商进行操作。
ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。没有任何类型代理被创建，这只有 Kubernetes 1.7 或更高版本的 kube-dns 才支持。








