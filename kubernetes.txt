查看token：kubeadm token list
生成token：kubeadm token create
获取CA证书hash：openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
排错：journalctl -f -u kubelet;kubectl describe nodes;systemctl status kubelet
卸载节点：kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
删除节点：kubectl delete node <node name>
重置节点：kubeadm reset
kubeadm join 192.168.110.128:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:62e5a13e71cbd6b51a71fb19799c4af651ce9004e60ba6bdbda9bdd7f7b699ca 
--------------------------------------------------简介-------------------------------------------------
*docker编排工具：docker compose(单机) docker swarm(整合接口) docker mechine(初始化主机加入docker swarm)
*kubernetes:service automation 
*Service是分布式集群架构的核心,基于socket通信。特征：Service是分布式集群架构的核心；拥有一个虚拟IP(Cluster IP、Service IP或VIP)和端口号；能够提供某种远程服务能力；被映射到提供这种服务能力的一组容器应用上。
*pod对象：将每个服务进程都包装到相应的Pod中,使其成为在Pod中运行的一个容器(Container)，pod贴label，service选择label关联pod
*Kubernetes将集群中的机器划分为一个Master和一些Node。在Master上运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kubescheduler,这些进程实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理功能,并且都是自动完成的。Node作为集群中的工作节点,运行真正的应用程序,在Node上Kubernetes管理的最小运行单元是Pod。在Node上运行着Kubernetes的kubelet、kube-proxy服务进程,这些服务进程负责Pod的创建、启动、监控、重启、销毁,以及实现软件模式的负载均衡器。
*Master：集群的管理节点，有一个或者一组节点，一般 3 个足够了。
nodes：提供计算资源的节点，就是运行容器的节点，可以扩展。
*在Kubernetes集群中,只需为需要扩容的Service关联的Pod创建一个RC(Replication Controller),RC定义文件中包括:目标Pod的定义;目标Pod需要运行的副本数量(Replicas); 要监控的目标Pod的标签
*ApiServer:kubernetes接收用户创建容器等请求的是Kubernetes Cluster，那么它对外提供服务的接口就是一个API 接口。包括认证授权、数据校验以及集群状态变更，以及负责其他模块直接的相互通讯和数据交互，只有api server才能操作etcd
*scheduler(调度器):如果客户请求是运行一个容器，那么Master会使用调度器（scheduler）根据用户的请求来分配一个能够运行容器的nodes节点
*Controller:如果运行容器的节点宕机或者容器本身运行出现问题，kubernetes能够在其他节点再启动一个一模一样的容器
*ControllerManager：如果控制器不健康无法工作，那么由控制器管理器来确保控制器的健康，由于Master有多个，所以具有冗余性。
*Pod（原子调度单元，是容器的封装）：在Kubernetes上调度的原子单元，Kubernetes不直接调度容器，而是Pod，Pod可以理解为容器的二次封装，可以由一个或者多个容器组成，多个容器共享同一个网络名称空间：NET、UTS、IPC。同一个 POD里的容器，还能共享同一个存储卷，存储卷可以属于POD。一般一个POD只运行一个容器，如果需要在POD放多个容器，那么一般有一个主容器，其他容器是为主容器提供服务的。
*node:提供计算资源的节点，就是运行Pod的主机，Kubenetes Cluster统一管理所有的node节点的计算资源，当用户请求
创建资源的时候，可以检查目前集群还有没有资源可以运行用户的容器，这实现了统一调度统一管理的一个平台。
*label:由 key = value 组成的标签，可以为 POD 打上一个标签。
*selector(标签选择器)
-------------------------------------------架构和组件---------------------------------------------
*etcd:用于Kubernetes的后端数据存储，所有集群数据都存储在此处
*Master 节点负责维护集群的目标状态，上面运行的主控组件有:kube-apiserver(对外暴露了 Kubernetes API，它是的 Kubernetes 前端控制层，只有 API Server 会与 etcd 通信，其它模块都必须通过 API Server 访问集群状态)
kube-controller-manager(处理集群中常规任务，它是单独的进程，内部包含多个控制器，例如维护 POD 数量)
kube-scheduler(监视新创建的 Pod 为新创建的POD分配合适的 node 节点)
*Node节点实际负责实施，也就是运行 POD 的节点，上面运行的组件有:kubelet(它监测已经分配给自己的 Pod，为 POD 准备卷，下载 POD 所需的 Secret，下载镜像并运行，进行生命周期探测，上报 POD 和节点状态）
kube-proxy（通过维护主机上的网络规则并执行连接转发，将Kubernetes提供的网络服务代理到每个节点上，实现了Kubernetes服务抽象） docker(用于运行容器)
*插件:插件是增强集群功能的 Pod 和 Service，插件对象本身是受命名空间限制的，被创建于 kube-system 命名空间。
*DNS:其他插件并不是必需的，但所有 Kubernetes 集群都应该具有Cluster DNS，许多应用依赖于它，为 Kubernetes 服务提供DNS记录，容器启动该后会自动将 DNS 服务器包含在 resolv.conf 中。
------------------------------------------核心组件-----------------------------------------------
*Controller:
RelicationController(控制同一类POD对象的副本数量，实现程序的滚动更新，或者回滚的操作。在滚动更新时候，允许临时超出规定的副本数量)
RelicaSet(副本集控制器，它不直接使用，它有一个声明式中心控制器 Deployment)
Deployment(它只能管理无状态的应用，支持二级控制器，例如：HPA（Horizontal Pod Autoscaler，水平 POD 自动伸缩控制器），当负载高的时候，自动启动更多的 POD。)
StatefulSet(管理有状态的应用)
DaeminSet(如果需要在每一个 node 上运行一个副本，而不是随意运行)
Job(运行作业，时间不固定的操作，例如：备份、清理，临时启动一个POD来进行备份的任务，运行完成就结束了,如果运行时候JOB挂了，那么需要重新启动起来，如果运行完成了则不需要再启动了。)
Cronjob(周期性作业)
*Service(为客户端提供一个稳定的访问入口，Service靠标签选择器来关联POD的，只要POD上有相关的标签，那么就会被 Service选中，作为Service的后端，Service关联POD后会动态探测这个POD的IP地址和端口，并作为自己调度的后端。 Service 只是规则，ping不通，可以通过 Service 的名称解析为 Service 的 IP 地址。):
AddOns(解析域名是由DNS来解析的，为k8s中提供域名解析这种基础服务，称之为基础架构POD也称为k8s附件。k8s 中的 dns 附件，是动态的。)
*网络模型：
POD网络：所有 POD 处于同一个网络中
集群网络：Service 是一个另外一个网络
节点网络：node 节点也是另外一个网络
接入外部访问时候，请求首先到达 node 网络，然后 node 网络代理至 service 网络，service 根据 ipvs规则来转发到 pod 网络中的 pod 上。
通信方式：同一个 POD 内的多个容器间的通信，可以直接通过lo通信;POD与POD通信，所有POD都处于一个网络，可以跨 node 与另外的 POD 直接通信，因为使用了叠加网络;POD 与 Service 通信。
*kube-proxy:在 node 节点上运行的一个守护进程，它负责随时与 apiserver 进行通信。kube-proxy 一旦发现某个 service 后端的 pod 地址发生改变，那么就由 kube-proxy 负责在本地将地址写入 iptables 或者 ipvs 规则中。
*etcd:整个集群的所有信息都保存在 etcd，所以 etcd 如果宕机，那么整个集群就挂了，因而 etcd 需要做高可用。
*flanel:托管为k8s的附件运行。node网络：各节点之间进行通信。POD网络：所有node上的POD彼此之间通过叠加，或者直接路由方式通信。service 网络：由 kube-proxy 负责管控和生成
----------------------------------------集群部署---------------------------------------------------
*关闭firewalld和selinux:
setenforce 0
sed -i '/^SELINUX=/cSELINUX=disabled' /etc/selinux/config
systemctl stop firewalld
systemctl disable firewalld
*加载ipvs:
yum -y install ipvsadm ipset sysstat conntrack libseccomp
cat >>/etc/modules-load.d/ipvs.conf<<EOF
ip_vs_dh
ip_vs_ftp
ip_vs
ip_vs_lblc
ip_vs_lblcr
ip_vs_lc
ip_vs_nq
ip_vs_pe_sip
ip_vs_rr
ip_vs_sed
ip_vs_sh
ip_vs_wlc
ip_vs_wrr
nf_conntrack_ipv4
EOF
systemctl enable systemd-modules-load.service   # 设置开机加载内核模块
lsmod | grep -e ip_vs -e nf_conntrack_ipv4      # 重启后检查 ipvs 模块是否加载
kubectl edit -n kube-system configmap kube-proxy  #修改 mode 为 ipvs 重启集群
*下载docker和k8s
curl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 
cat >>/etc/yum.repos.d/kuberetes.repo<<EOF
[kuberneres]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
enabled=1
EOF
yum install docker-ce kubelet kubectl kubeadm -y
systemctl start docker
systemctl enable docker
systemctl enable kubelet
*设置内核及k8s参数
cat >>/etc/sysctl.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
KUBE_PROXY_MODE=ipvs
EOF
*部署master
vim /usr/lib/systemd/system/docker.service  # Docker 拉取镜像时候的代理地址
[Service]
Environment="HTTPS_PROXY=127.0.0.1:9666"
Environment="NO_PROXY=127.0.0.0/8,172.16.0.0/16"

kubeadm config images pull  # 提前拉取初始化需要的镜像
kubeadm config images list  # 查看需要的docker image name
docker pull mirrorgooglecontainers/image_name
docker rmi `docker images |grep docker.io/ |awk '{print $1":"$2}'`

*初始化master
kubeadm init --kubernetes-version=v1.14.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap  #初始化集群
journalctl -xeu kubelet # 查看报错
kubeadm reset # 初始化过程被中断时恢复
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl get pods -n kube-system
kubectl get ComponentStatus
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
*部署node:
kubeadm join 172.16.100.9:6443 --token 2dyd69.hrfsjkkxs4stim7n \
    --discovery-token-ca-cert-hash sha256:4e30c1f41aefb177b708a404ccb7e818e31647c7dbdd2d42f6c5c9894b6f41e7 --ignore-preflight-errors=Swap
docker image ls
kubectl get nodes
kubectl cluster-info
kubectl get componentstatuses
kubectl get pods -n kube-system -o wide
*镜像太慢
docker image save -o /tmp/kube-proxy.tar k8s.gcr.io/kube-proxy  #master导出
docker image save -o /tmp/flannel.tar quay.io/coreos/flannel
docker image save -o /tmp/pause.tar k8s.gcr.io/pause
docker image load -i /tmp/kube-proxy.tar  # load导入
docker image load -i /tmp/pause.tar
docker image load -i /tmp/flannel.tar

----------------------------------------------------------PV---------------------------------------
PV持久化卷   PVC(持久化卷声明)，PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式
StorageClass可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储








